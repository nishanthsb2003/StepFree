{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCKPTPH0MXxH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# emg_train_with_augmentation.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# -----------------------\n",
        "# USER PARAMETERS\n",
        "# -----------------------\n",
        "csv_paths = [\"data/session_rest.csv\", \"data/session_walk.csv\", \"data/session_lift.csv\"]\n",
        "emg_col = \"emg\"\n",
        "label_col = \"label\"\n",
        "sampling_rate = 1000   # Hz\n",
        "window_ms = 50         # smaller so short files give windows (50 ms => 50 samples @1000Hz)\n",
        "step_ms = 25           # overlap step (50% overlap)\n",
        "target_windows_per_class = 200   # how many windows per class after augmentation (set lower if you want)\n",
        "min_real_windows_required = 1    # must have at least 1 real window per class to augment from\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "random_seed = 42\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "# -----------------------\n",
        "# helpers: label inference, loading, windowing\n",
        "# -----------------------\n",
        "def infer_label_from_filename(path):\n",
        "    fname = os.path.basename(path).lower()\n",
        "    if \"rest\" in fname: return \"rest\"\n",
        "    if \"walk\" in fname: return \"walk\"\n",
        "    if \"lift\" in fname or \"flex\" in fname: return \"lift\"\n",
        "    return os.path.splitext(fname)[0]\n",
        "\n",
        "def load_single_file(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if label_col not in df.columns or df[label_col].isna().all():\n",
        "        df[label_col] = infer_label_from_filename(path)\n",
        "    return df\n",
        "\n",
        "def build_windows_from_array(arr_emg, win_samples, step_samples):\n",
        "    n = len(arr_emg)\n",
        "    windows = []\n",
        "    for start in range(0, n - win_samples + 1, step_samples):\n",
        "        windows.append(arr_emg[start:start+win_samples].astype(np.float32))\n",
        "    return np.array(windows)\n",
        "\n",
        "# -----------------------\n",
        "# Augmentation functions (simple, realistic)\n",
        "# -----------------------\n",
        "def augment_window(window, max_jitter=5, noise_std=0.02, scale_range=(0.9, 1.1)):\n",
        "    \"\"\"Return a single augmented version of window.\"\"\"\n",
        "    w = window.copy()\n",
        "    L = len(w)\n",
        "\n",
        "    # 1) random jitter: shift left/right by up to max_jitter samples (pad with edge values)\n",
        "    shift = np.random.randint(-max_jitter, max_jitter+1)\n",
        "    if shift > 0:\n",
        "        w = np.concatenate([w[shift:], np.full(shift, w[-1], dtype=w.dtype)])\n",
        "    elif shift < 0:\n",
        "        s = -shift\n",
        "        w = np.concatenate([np.full(s, w[0], dtype=w.dtype), w[:-s]])\n",
        "\n",
        "    # 2) amplitude scaling\n",
        "    scale = np.random.uniform(scale_range[0], scale_range[1])\n",
        "    w = w * scale\n",
        "\n",
        "    # 3) additive gaussian noise (relative to signal range)\n",
        "    sig_range = np.max(w) - np.min(w)\n",
        "    noise = np.random.normal(0.0, noise_std * (sig_range + 1e-8), size=w.shape)\n",
        "    w = w + noise\n",
        "\n",
        "    # 4) small DC offset\n",
        "    offset = np.random.normal(0.0, 0.01 * (sig_range + 1e-8))\n",
        "    w = w + offset\n",
        "\n",
        "    return w\n",
        "\n",
        "def create_augmented_windows(real_windows, target_count):\n",
        "    \"\"\"Given array(real_windows, win_samples), generate target_count windows.\"\"\"\n",
        "    if len(real_windows) == 0:\n",
        "        return np.zeros((0,0), dtype=np.float32)\n",
        "    out = []\n",
        "    n_real = len(real_windows)\n",
        "    # always include real windows first (shuffled)\n",
        "    indices = list(range(n_real))\n",
        "    random.shuffle(indices)\n",
        "    for i in indices:\n",
        "        out.append(real_windows[i])\n",
        "    # augment until reaching target_count\n",
        "    while len(out) < target_count:\n",
        "        # pick a random real window as base\n",
        "        base = real_windows[np.random.randint(0, n_real)]\n",
        "        new_w = augment_window(base)\n",
        "        out.append(new_w)\n",
        "    return np.array(out, dtype=np.float32)\n",
        "\n",
        "# -----------------------\n",
        "# Build windows per-file, then augment per-class\n",
        "# -----------------------\n",
        "win_samples = int(sampling_rate * window_ms / 1000)\n",
        "step_samples = int(sampling_rate * step_ms / 1000)\n",
        "if win_samples <= 0:\n",
        "    raise ValueError(\"window_ms too small for given sampling_rate\")\n",
        "\n",
        "print(\"Window params:\", win_samples, \"samples per window ; step:\", step_samples)\n",
        "\n",
        "per_class_windows = {}   # class -> list of windows\n",
        "\n",
        "for p in csv_paths:\n",
        "    if not os.path.exists(p):\n",
        "        raise FileNotFoundError(f\"Missing file: {p}\")\n",
        "    df = load_single_file(p)\n",
        "    label = str(df[label_col].iloc[0])\n",
        "    emg = df[emg_col].values\n",
        "    windows = build_windows_from_array(emg, win_samples, step_samples)\n",
        "    print(f\"{os.path.basename(p)} -> rows: {len(emg)} ; windows: {len(windows)} ; label: {label}\")\n",
        "    per_class_windows.setdefault(label, []).extend(list(windows))\n",
        "\n",
        "# ensure all three classes present (inform user if missing)\n",
        "print(\"\\nClasses found and real-window counts:\")\n",
        "for cls, arr in per_class_windows.items():\n",
        "    print(\" \", cls, \":\", len(arr))\n",
        "\n",
        "required_classes = [\"rest\", \"walk\", \"lift\"]\n",
        "for rc in required_classes:\n",
        "    if rc not in per_class_windows:\n",
        "        print(f\"[ERROR] Class '{rc}' missing entirely. You need at least some real recordings for this class.\")\n",
        "        # do NOT attempt to fabricate an entire class from nothing\n",
        "        # exit early\n",
        "        raise ValueError(f\"Missing required class: {rc}\")\n",
        "\n",
        "# check minimum real windows\n",
        "for cls, arr in per_class_windows.items():\n",
        "    if len(arr) < min_real_windows_required:\n",
        "        raise ValueError(f\"Class '{cls}' has {len(arr)} real windows (< {min_real_windows_required}). Record at least {min_real_windows_required} windows before augmenting.\")\n",
        "\n",
        "# Augment to target count per class\n",
        "X_aug = []\n",
        "y_aug = []\n",
        "for cls, arr in per_class_windows.items():\n",
        "    arr_np = np.array(arr)  # shape (n_real, win_samples)\n",
        "    print(f\"[INFO] Augmenting class '{cls}': real {len(arr_np)} -> target {target_windows_per_class}\")\n",
        "    augmented = create_augmented_windows(arr_np, target_windows_per_class)\n",
        "    X_aug.append(augmented)\n",
        "    y_aug.extend([cls] * len(augmented))\n",
        "\n",
        "X = np.vstack(X_aug)\n",
        "y = np.array(y_aug)\n",
        "print(\"Total windows after augmentation:\", X.shape)\n",
        "\n",
        "# -----------------------\n",
        "# Prepare for training\n",
        "# -----------------------\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "class_names = le.classes_\n",
        "num_classes = len(class_names)\n",
        "print(\"Class names:\", class_names)\n",
        "\n",
        "# reshape for Keras\n",
        "X = X[..., np.newaxis]\n",
        "\n",
        "# Standardize (fit on X)\n",
        "mean = X.mean()\n",
        "std = X.std()\n",
        "X = (X - mean) / (std + 1e-8)\n",
        "\n",
        "# Train/val/test split (stratify)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y_enc, test_size=0.25, random_state=random_seed, stratify=y_enc)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_seed, stratify=y_temp)\n",
        "\n",
        "print(\"Shapes -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# Build model (same 1D-CNN)\n",
        "# -----------------------\n",
        "input_shape = X_train.shape[1:]\n",
        "def build_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv1D(32, 5, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 5, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(input_shape, num_classes)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_emg_model.h5\", save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate & save\n",
        "# -----------------------\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test acc:\", test_acc)\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "model.save(\"final_emg_model.h5\")\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"emg_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "print(\"Saved final_emg_model.h5 and emg_model.tflite\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HK0pg702M5aT",
        "outputId": "83f3e92e-6555-447b-e01f-9f4c9578227d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window params: 50 samples per window ; step: 25\n",
            "session_rest.csv -> rows: 101 ; windows: 3 ; label: rest\n",
            "session_walk.csv -> rows: 52 ; windows: 1 ; label: walk\n",
            "session_lift.csv -> rows: 50 ; windows: 1 ; label: lift\n",
            "\n",
            "Classes found and real-window counts:\n",
            "  rest : 3\n",
            "  walk : 1\n",
            "  lift : 1\n",
            "[INFO] Augmenting class 'rest': real 3 -> target 200\n",
            "[INFO] Augmenting class 'walk': real 1 -> target 200\n",
            "[INFO] Augmenting class 'lift': real 1 -> target 200\n",
            "Total windows after augmentation: (600, 50)\n",
            "Class names: ['lift' 'rest' 'walk']\n",
            "Shapes -> train: (450, 50, 1) val: (75, 50, 1) test: (75, 50, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m10,304\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,547\u001b[0m (174.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,547</span> (174.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,099\u001b[0m (172.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,099</span> (172.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 3s - 436ms/step - accuracy: 0.9022 - loss: 0.3135 - val_accuracy: 0.6667 - val_loss: 0.9675\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.0223 - val_accuracy: 0.6667 - val_loss: 0.8763\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 35ms/step - accuracy: 1.0000 - loss: 0.0102 - val_accuracy: 0.6667 - val_loss: 0.8072\n",
            "Epoch 4/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.6800 - val_loss: 0.7509\n",
            "Epoch 5/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 32ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.8133 - val_loss: 0.7013\n",
            "Epoch 6/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 50ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.8933 - val_loss: 0.6557\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 51ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9200 - val_loss: 0.6155\n",
            "Epoch 8/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 50ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9333 - val_loss: 0.5803\n",
            "Epoch 9/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 51ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9467 - val_loss: 0.5471\n",
            "Epoch 10/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 51ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9467 - val_loss: 0.5138\n",
            "Epoch 11/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 60ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9600 - val_loss: 0.4824\n",
            "Epoch 12/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 34ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9600 - val_loss: 0.4515\n",
            "Epoch 13/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 32ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9600 - val_loss: 0.4206\n",
            "Epoch 14/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 31ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9600 - val_loss: 0.3901\n",
            "Epoch 15/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 7.5981e-04 - val_accuracy: 0.9600 - val_loss: 0.3593\n",
            "Epoch 16/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9600 - val_loss: 0.3298\n",
            "Epoch 17/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 32ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9600 - val_loss: 0.3012\n",
            "Epoch 18/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 8.7278e-04 - val_accuracy: 0.9600 - val_loss: 0.2741\n",
            "Epoch 19/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 34ms/step - accuracy: 1.0000 - loss: 8.5246e-04 - val_accuracy: 0.9600 - val_loss: 0.2486\n",
            "Epoch 20/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 34ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9867 - val_loss: 0.2237\n",
            "Epoch 21/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 32ms/step - accuracy: 1.0000 - loss: 8.2477e-04 - val_accuracy: 0.9867 - val_loss: 0.2001\n",
            "Epoch 22/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 34ms/step - accuracy: 1.0000 - loss: 5.1289e-04 - val_accuracy: 0.9867 - val_loss: 0.1772\n",
            "Epoch 23/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 34ms/step - accuracy: 1.0000 - loss: 8.3586e-04 - val_accuracy: 0.9867 - val_loss: 0.1562\n",
            "Epoch 24/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 8.2539e-04 - val_accuracy: 0.9867 - val_loss: 0.1390\n",
            "Epoch 25/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 6.0533e-04 - val_accuracy: 0.9867 - val_loss: 0.1215\n",
            "Epoch 26/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 7.0312e-04 - val_accuracy: 0.9867 - val_loss: 0.1046\n",
            "Epoch 27/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 34ms/step - accuracy: 1.0000 - loss: 4.9961e-04 - val_accuracy: 0.9867 - val_loss: 0.0900\n",
            "Epoch 28/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 36ms/step - accuracy: 1.0000 - loss: 3.8618e-04 - val_accuracy: 0.9867 - val_loss: 0.0778\n",
            "Epoch 29/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 3.5496e-04 - val_accuracy: 0.9867 - val_loss: 0.0667\n",
            "Epoch 30/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 - 0s - 33ms/step - accuracy: 1.0000 - loss: 3.7115e-04 - val_accuracy: 0.9867 - val_loss: 0.0574\n",
            "Test acc: 0.9866666793823242\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        lift       1.00      0.96      0.98        25\n",
            "        rest       0.96      1.00      0.98        25\n",
            "        walk       1.00      1.00      1.00        25\n",
            "\n",
            "    accuracy                           0.99        75\n",
            "   macro avg       0.99      0.99      0.99        75\n",
            "weighted avg       0.99      0.99      0.99        75\n",
            "\n",
            "Confusion matrix:\n",
            " [[24  1  0]\n",
            " [ 0 25  0]\n",
            " [ 0  0 25]]\n",
            "Saved artifact at '/tmp/tmpb_zqdqea'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name='keras_tensor_13')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135768143558224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637998096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637998480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637999440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637999248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637998672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637999056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637997136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637997712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638000592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638000400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637997520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638000208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637999632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637998288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638001744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638001552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768637999824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638001360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638000784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638002512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768638001936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Saved final_emg_model.h5 and emg_model.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_tflite_to_c_header.py\n",
        "from tensorflow.lite.python.util import convert_bytes_to_c_source\n",
        "# If you saved emg_model.tflite earlier:\n",
        "with open(\"emg_model.tflite\", \"rb\") as f:\n",
        "    tflite_bytes = f.read()\n",
        "\n",
        "# convert -> returns (c_src, header_src)\n",
        "c_src, h_src = convert_bytes_to_c_source(tflite_bytes, \"my_model\")\n",
        "\n",
        "# Rename to TF-Micro naming\n",
        "h_src = h_src.replace('unsigned char my_model[]', 'unsigned char g_my_model[]')\n",
        "h_src = h_src.replace('unsigned int my_model_len', 'unsigned int g_my_model_len')\n",
        "\n",
        "# Write header\n",
        "with open(\"emg_model.h\", \"w\") as fh:\n",
        "    fh.write(h_src)\n",
        "\n",
        "print(\"Wrote emg_model.h with g_my_model[], g_my_model_len\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvez1Ro6OAyb",
        "outputId": "f964ca5b-f40b-4716-f281-85d5fa48b6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote emg_model.h with g_my_model[], g_my_model_len\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tflite_evaluate_real_windows.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# -----------------------\n",
        "# USER PARAMETERS (change if needed)\n",
        "# -----------------------\n",
        "csv_paths = [\"data/session_rest.csv\", \"data/session_walk.csv\", \"data/session_lift.csv\"]\n",
        "emg_col = \"emg\"\n",
        "label_col = \"label\"   # if missing in CSV, script will infer label from filename\n",
        "sampling_rate = 1000   # Hz used to compute samples from window_ms\n",
        "window_ms = 50         # must match window size you used in training (50 ms -> 50 samples @1000Hz)\n",
        "step_ms = 25\n",
        "tflite_path = \"emg_model.tflite\"\n",
        "mean_path = \"mean.npy\"\n",
        "std_path = \"std.npy\"\n",
        "classes_path = \"classes.npy\"\n",
        "save_predictions_csv = \"tflite_predictions.csv\"\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "def infer_label_from_filename(path):\n",
        "    fname = os.path.basename(path).lower()\n",
        "    if \"rest\" in fname: return \"rest\"\n",
        "    if \"walk\" in fname: return \"walk\"\n",
        "    if \"lift\" in fname or \"flex\" in fname: return \"lift\"\n",
        "    return os.path.splitext(fname)[0]\n",
        "\n",
        "def load_single_file(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if label_col not in df.columns or df[label_col].isna().all():\n",
        "        df[label_col] = infer_label_from_filename(path)\n",
        "    return df\n",
        "\n",
        "def build_windows_from_array(arr_emg, win_samples, step_samples):\n",
        "    windows = []\n",
        "    n = len(arr_emg)\n",
        "    for start in range(0, n - win_samples + 1, step_samples):\n",
        "        windows.append(arr_emg[start:start+win_samples].astype(np.float32))\n",
        "    return np.array(windows)\n",
        "\n",
        "# -----------------------\n",
        "# Build real windows & labels (no augmentation)\n",
        "# -----------------------\n",
        "win_samples = int(sampling_rate * window_ms / 1000)\n",
        "step_samples = int(sampling_rate * step_ms / 1000)\n",
        "if win_samples <= 0:\n",
        "    raise ValueError(\"Invalid window settings: win_samples <= 0\")\n",
        "\n",
        "print(\"Window size (samples):\", win_samples, \" step:\", step_samples)\n",
        "all_windows = []\n",
        "all_labels = []\n",
        "file_info = []\n",
        "\n",
        "for p in csv_paths:\n",
        "    if not os.path.exists(p):\n",
        "        print(f\"[WARN] File not found: {p} (skipping)\")\n",
        "        continue\n",
        "    df = load_single_file(p)\n",
        "    label = str(df[label_col].iloc[0])\n",
        "    arr = df[emg_col].values\n",
        "    windows = build_windows_from_array(arr, win_samples, step_samples)\n",
        "    print(f\"Loaded {os.path.basename(p)} -> rows: {len(arr)} ; windows: {len(windows)} ; label: {label}\")\n",
        "    for w in windows:\n",
        "        all_windows.append(w)\n",
        "        all_labels.append(label)\n",
        "    file_info.append((p, label, len(arr), len(windows)))\n",
        "\n",
        "if len(all_windows) == 0:\n",
        "    raise ValueError(\"No real windows built from CSVs. Try reducing window_ms or increasing overlap (step_ms).\")\n",
        "\n",
        "all_windows = np.array(all_windows)  # shape (N, win_samples)\n",
        "all_labels = np.array(all_labels)    # shape (N,)\n",
        "\n",
        "print(\"Total real windows:\", all_windows.shape[0])\n",
        "print(\"Raw label counts:\", pd.Series(all_labels).value_counts().to_dict())\n",
        "\n",
        "# -----------------------\n",
        "# Load mean/std/classes if available; fallback compute from real windows\n",
        "# -----------------------\n",
        "if os.path.exists(mean_path) and os.path.exists(std_path) and os.path.exists(classes_path):\n",
        "    mean = np.load(mean_path).item() if os.path.exists(mean_path) else np.load(mean_path)\n",
        "    std  = np.load(std_path).item()  if os.path.exists(std_path) else np.load(std_path)\n",
        "    classes = np.load(classes_path, allow_pickle=True)\n",
        "    print(\"[INFO] Loaded mean/std/classes from files.\")\n",
        "else:\n",
        "    print(\"[WARN] mean/std/classes not all found. Computing mean/std from real windows (FALLBACK).\")\n",
        "    mean = float(all_windows.mean())\n",
        "    std  = float(all_windows.std())\n",
        "    # infer classes from labels present\n",
        "    classes = np.unique(all_labels)\n",
        "    print(f\"[INFO] Inferred classes = {classes}\")\n",
        "    # Save these so you can reuse downstream\n",
        "    np.save(\"mean_inferred.npy\", mean)\n",
        "    np.save(\"std_inferred.npy\", std)\n",
        "    np.save(\"classes_inferred.npy\", classes)\n",
        "    print(\"[INFO] Saved mean_inferred.npy, std_inferred.npy, classes_inferred.npy\")\n",
        "\n",
        "# Build mapping label -> index using classes order (important to match training order if available)\n",
        "le = LabelEncoder()\n",
        "le.fit(classes)\n",
        "y_true = le.transform(all_labels)  # integers 0..C-1\n",
        "class_names = le.classes_\n",
        "print(\"Class names used (order):\", class_names)\n",
        "\n",
        "# -----------------------\n",
        "# Load TFLite model\n",
        "# -----------------------\n",
        "if not os.path.exists(tflite_path):\n",
        "    raise FileNotFoundError(f\"TFLite model not found at {tflite_path}\")\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(\"TFLite input shape:\", input_details[0]['shape'], \" dtype:\", input_details[0]['dtype'])\n",
        "print(\"TFLite output shape:\", output_details[0]['shape'], \" dtype:\", output_details[0]['dtype'])\n",
        "\n",
        "# -----------------------\n",
        "# Inference loop over real windows\n",
        "# -----------------------\n",
        "y_pred = []\n",
        "y_pred_probs = []\n",
        "\n",
        "for i, w in enumerate(all_windows):\n",
        "    # preprocess: normalize with mean/std\n",
        "    x = (w.astype(np.float32) - mean) / (std + 1e-8)\n",
        "    x = x.reshape(1, win_samples, 1)\n",
        "\n",
        "    # If the tflite model expects a different dtype (e.g., int8), cast accordingly\n",
        "    # We will attempt to cast to input_details dtype if needed:\n",
        "    if input_details[0]['dtype'] == np.int8 or input_details[0]['dtype'] == np.uint8:\n",
        "        # scale float [-1,1] to int8 using quantization params if present\n",
        "        # check if quantization params are available\n",
        "        scale, zero_point = input_details[0].get('quantization', (None, None))\n",
        "        if scale and zero_point is not None:\n",
        "            x_int = (x / scale + zero_point).astype(input_details[0]['dtype'])\n",
        "            interpreter.set_tensor(input_details[0]['index'], x_int)\n",
        "        else:\n",
        "            # fallback: cast float32 to required dtype\n",
        "            interpreter.set_tensor(input_details[0]['index'], x.astype(input_details[0]['dtype']))\n",
        "    else:\n",
        "        interpreter.set_tensor(input_details[0]['index'], x.astype(np.float32))\n",
        "\n",
        "    interpreter.invoke()\n",
        "    out = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "\n",
        "    # If output is quantized, dequantize if needed\n",
        "    if output_details[0]['dtype'] == np.int8 or output_details[0]['dtype'] == np.uint8:\n",
        "        scale_o, zp_o = output_details[0].get('quantization', (None, None))\n",
        "        if scale_o and zp_o is not None:\n",
        "            out = (out.astype(np.float32) - zp_o) * scale_o\n",
        "\n",
        "    probs = np.asarray(out)\n",
        "    pred_idx = int(np.argmax(probs))\n",
        "    y_pred.append(pred_idx)\n",
        "    y_pred_probs.append(probs)\n",
        "\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_probs = np.vstack(y_pred_probs)\n",
        "\n",
        "# -----------------------\n",
        "# Metrics & Reporting\n",
        "# -----------------------\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nOverall accuracy: {acc:.4f}  |  Balanced accuracy: {bal_acc:.4f}\\n\")\n",
        "\n",
        "print(\"Classification report (per-class):\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "print(\"Confusion matrix:\\n\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Per-class accuracy\n",
        "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "for i, n in enumerate(class_names):\n",
        "    print(f\"Class {n}: accuracy {per_class_acc[i]:.3f} (support {cm.sum(axis=1)[i]})\")\n",
        "\n",
        "# -----------------------\n",
        "# Save predictions CSV for inspection\n",
        "# -----------------------\n",
        "rows = []\n",
        "for i in range(len(all_windows)):\n",
        "    true_label = all_labels[i]\n",
        "    pred_label = class_names[y_pred[i]]\n",
        "    probs = y_pred_probs[i]\n",
        "    rows.append({\n",
        "        \"file\": None,\n",
        "        \"true_label\": true_label,\n",
        "        \"pred_label\": pred_label,\n",
        "        \"pred_idx\": int(y_pred[i]),\n",
        "        \"probabilities\": \",\".join([f\"{p:.6f}\" for p in probs])\n",
        "    })\n",
        "\n",
        "df_out = pd.DataFrame(rows)\n",
        "df_out.to_csv(save_predictions_csv, index=False)\n",
        "print(f\"\\nSaved predictions to {save_predictions_csv}\")\n",
        "\n",
        "# -----------------------\n",
        "# Done\n",
        "# -----------------------\n",
        "print(\"\\nFinished TFLite evaluation on real windows.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj36nAb7Pffa",
        "outputId": "385ac983-7775-4df4-e632-38831ee5ed45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window size (samples): 50  step: 25\n",
            "Loaded session_rest.csv -> rows: 101 ; windows: 3 ; label: rest\n",
            "Loaded session_walk.csv -> rows: 52 ; windows: 1 ; label: walk\n",
            "Loaded session_lift.csv -> rows: 50 ; windows: 1 ; label: lift\n",
            "Total real windows: 5\n",
            "Raw label counts: {'rest': 3, 'walk': 1, 'lift': 1}\n",
            "[WARN] mean/std/classes not all found. Computing mean/std from real windows (FALLBACK).\n",
            "[INFO] Inferred classes = ['lift' 'rest' 'walk']\n",
            "[INFO] Saved mean_inferred.npy, std_inferred.npy, classes_inferred.npy\n",
            "Class names used (order): ['lift' 'rest' 'walk']\n",
            "TFLite input shape: [ 1 50  1]  dtype: <class 'numpy.float32'>\n",
            "TFLite output shape: [1 3]  dtype: <class 'numpy.float32'>\n",
            "\n",
            "Overall accuracy: 1.0000  |  Balanced accuracy: 1.0000\n",
            "\n",
            "Classification report (per-class):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        lift       1.00      1.00      1.00         1\n",
            "        rest       1.00      1.00      1.00         3\n",
            "        walk       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         5\n",
            "   macro avg       1.00      1.00      1.00         5\n",
            "weighted avg       1.00      1.00      1.00         5\n",
            "\n",
            "Confusion matrix:\n",
            "\n",
            "[[1 0 0]\n",
            " [0 3 0]\n",
            " [0 0 1]]\n",
            "Class lift: accuracy 1.000 (support 1)\n",
            "Class rest: accuracy 1.000 (support 3)\n",
            "Class walk: accuracy 1.000 (support 1)\n",
            "\n",
            "Saved predictions to tflite_predictions.csv\n",
            "\n",
            "Finished TFLite evaluation on real windows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# -----------------------------\n",
        "# USER SETTINGS\n",
        "# -----------------------------\n",
        "csv_paths = [\n",
        "    \"data/session_rest.csv\",\n",
        "    \"data/session_walk.csv\",\n",
        "    \"data/session_lift.csv\"\n",
        "]\n",
        "\n",
        "emg_col = \"emg\"\n",
        "sampling_rate = 1000    # Hz\n",
        "window_ms = 50          # You used 50ms windows\n",
        "step_ms = 25            # 50% overlap\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD mean / std / classes\n",
        "# -----------------------------\n",
        "mean = np.load(\"mean_inferred.npy\")\n",
        "std  = np.load(\"std_inferred.npy\")\n",
        "classes = np.load(\"classes_inferred.npy\", allow_pickle=True)\n",
        "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "# -----------------------------\n",
        "# LOAD TFLITE MODEL\n",
        "# -----------------------------\n",
        "interpreter = tf.lite.Interpreter(model_path=\"emg_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# -----------------------------\n",
        "# WINDOWING FUNCTION\n",
        "# -----------------------------\n",
        "win_samples = int(sampling_rate * window_ms / 1000)\n",
        "step_samples = int(sampling_rate * step_ms / 1000)\n",
        "\n",
        "def build_windows(arr):\n",
        "    windows = []\n",
        "    for start in range(0, len(arr) - win_samples + 1, step_samples):\n",
        "        windows.append(arr[start:start+win_samples].astype(np.float32))\n",
        "    return np.array(windows)\n",
        "\n",
        "# -----------------------------\n",
        "# BUILD TEST WINDOWS\n",
        "# -----------------------------\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for path in csv_paths:\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # infer class from filename\n",
        "    fname = path.lower()\n",
        "    if \"rest\" in fname: label = \"rest\"\n",
        "    elif \"walk\" in fname: label = \"walk\"\n",
        "    elif \"lift\" in fname or \"flex\" in fname: label = \"lift\"\n",
        "    else: label = \"unknown\"\n",
        "\n",
        "    arr = df[emg_col].values\n",
        "    wins = build_windows(arr)\n",
        "\n",
        "    for w in wins:\n",
        "        X.append(w)\n",
        "        Y.append(class_to_idx[label])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "print(\"Total windows built:\", len(X))\n",
        "\n",
        "# -----------------------------\n",
        "# RUN TFLITE INFERENCE\n",
        "# -----------------------------\n",
        "y_pred = []\n",
        "\n",
        "for w in X:\n",
        "    # normalize\n",
        "    w_norm = (w - mean) / (std + 1e-8)\n",
        "    w_norm = w_norm.reshape(1, win_samples, 1).astype(np.float32)\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], w_norm)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    out = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "    pred = np.argmax(out)\n",
        "    y_pred.append(pred)\n",
        "\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# -----------------------------\n",
        "# PRINT ACCURACY\n",
        "# -----------------------------\n",
        "print(\"\\n==============================\")\n",
        "print(\" ✔ MODEL ACCURACY RESULTS\")\n",
        "print(\"==============================\")\n",
        "\n",
        "print(\"\\nOverall Accuracy:\", round(accuracy_score(Y, y_pred), 4))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y, y_pred, target_names=classes))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6UUYYlhPqfi",
        "outputId": "a6d25d54-e85d-4c41-9913-1dfb31bb4347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['lift' 'rest' 'walk']\n",
            "Total windows built: 5\n",
            "\n",
            "==============================\n",
            " ✔ MODEL ACCURACY RESULTS\n",
            "==============================\n",
            "\n",
            "Overall Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        lift       1.00      1.00      1.00         1\n",
            "        rest       1.00      1.00      1.00         3\n",
            "        walk       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         5\n",
            "   macro avg       1.00      1.00      1.00         5\n",
            "weighted avg       1.00      1.00      1.00         5\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0 0]\n",
            " [0 3 0]\n",
            " [0 0 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrain_with_holdout_flexible.py  (copy & run)\n",
        "import os, random, numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "# --- user params (adjust if needed) ---\n",
        "csv_paths = [\"data/session_rest.csv\", \"data/session_walk.csv\", \"data/session_lift.csv\"]\n",
        "emg_col = \"emg\"\n",
        "label_col = \"label\"\n",
        "sampling_rate = 1000\n",
        "window_ms = 50\n",
        "step_ms = 25\n",
        "target_windows_per_class = 100   # lower if you want faster runs\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "random_seed = 42\n",
        "\n",
        "np.random.seed(random_seed); random.seed(random_seed); tf.random.set_seed(random_seed)\n",
        "\n",
        "# --- helpers ---\n",
        "def infer_label_from_filename(path):\n",
        "    fn = os.path.basename(path).lower()\n",
        "    if \"rest\" in fn: return \"rest\"\n",
        "    if \"walk\" in fn: return \"walk\"\n",
        "    if \"lift\" in fn or \"flex\" in fn: return \"lift\"\n",
        "    return os.path.splitext(fn)[0]\n",
        "\n",
        "def load_single_file(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if label_col not in df.columns or df[label_col].isna().all():\n",
        "        df[label_col] = infer_label_from_filename(path)\n",
        "    return df\n",
        "\n",
        "def build_windows(arr, win_samples, step_samples):\n",
        "    outs=[]\n",
        "    n=len(arr)\n",
        "    for s in range(0, n - win_samples + 1, step_samples):\n",
        "        outs.append(arr[s:s+win_samples].astype(np.float32))\n",
        "    return np.array(outs)\n",
        "\n",
        "def augment_window(w):\n",
        "    # small jitter, scale and noise\n",
        "    w = w.copy()\n",
        "    shift = np.random.randint(-3, 4)\n",
        "    if shift>0: w = np.concatenate([w[shift:], np.full(shift, w[-1], dtype=w.dtype)])\n",
        "    elif shift<0:\n",
        "        s=-shift; w = np.concatenate([np.full(s, w[0], dtype=w.dtype), w[:-s]])\n",
        "    w = w * np.random.uniform(0.95, 1.05)\n",
        "    rng = np.max(w) - np.min(w)\n",
        "    w = w + np.random.normal(0, 0.02*(rng+1e-8), size=w.shape)\n",
        "    return w\n",
        "\n",
        "def create_augmented(bases, target):\n",
        "    if len(bases)==0: return np.zeros((0,0), dtype=np.float32)\n",
        "    out = [b for b in bases]\n",
        "    while len(out) < target:\n",
        "        b = bases[np.random.randint(0, len(bases))]\n",
        "        out.append(augment_window(b))\n",
        "    return np.array(out, dtype=np.float32)\n",
        "\n",
        "# --- compute window sizes ---\n",
        "win_samples = int(sampling_rate * window_ms / 1000)\n",
        "step_samples = int(sampling_rate * step_ms / 1000)\n",
        "if win_samples <= 0: raise SystemExit(\"Invalid window settings\")\n",
        "\n",
        "print(\"Window samples:\", win_samples, \" step:\", step_samples)\n",
        "\n",
        "per_class_bases = {}\n",
        "per_class_test = {}\n",
        "\n",
        "for p in csv_paths:\n",
        "    if not os.path.exists(p):\n",
        "        print(\"[WARN] missing file:\", p); continue\n",
        "    df = load_single_file(p)\n",
        "    cls = str(df[label_col].iloc[0])\n",
        "    arr = df[emg_col].values\n",
        "    wins = build_windows(arr, win_samples, step_samples)\n",
        "    print(f\"{os.path.basename(p)} rows:{len(arr)} windows:{len(wins)} class:{cls}\")\n",
        "    if len(wins) == 0:\n",
        "        continue\n",
        "    if len(wins) >= 2:\n",
        "        # reserve last as test\n",
        "        per_class_test.setdefault(cls, []).append(wins[-1])\n",
        "        per_class_bases.setdefault(cls, []).extend(list(wins[:-1]))\n",
        "    else:\n",
        "        # exactly 1: use as augmentation base, no held-out\n",
        "        per_class_bases.setdefault(cls, []).extend(list(wins))\n",
        "        print(f\"[INFO] only 1 window for class '{cls}' — used as base, no held-out reserved.\")\n",
        "\n",
        "print(\"\\nBases per class:\")\n",
        "for c,v in per_class_bases.items(): print(\" \", c, len(v))\n",
        "print(\"\\nHeld-out per class:\")\n",
        "for c,v in per_class_test.items(): print(\" \", c, len(v))\n",
        "\n",
        "no_heldout = [c for c in per_class_bases.keys() if c not in per_class_test]\n",
        "if no_heldout:\n",
        "    print(\"\\n[WARN] Classes with NO held-out windows (cannot evaluate on unseen real windows):\", no_heldout)\n",
        "\n",
        "# --- create augmented training data ---\n",
        "X_train_list=[]; y_train_list=[]\n",
        "X_test=[]; y_test=[]\n",
        "for cls, bases in per_class_bases.items():\n",
        "    bases_np = np.array(bases) if len(bases)>0 else np.zeros((0,win_samples))\n",
        "    print(f\"[AUG] {cls}: bases={len(bases_np)} -> target {target_windows_per_class}\")\n",
        "    aug = create_augmented(bases_np, target_windows_per_class)\n",
        "    X_train_list.append(aug)\n",
        "    y_train_list.extend([cls]*len(aug))\n",
        "    for tw in per_class_test.get(cls, []):\n",
        "        X_test.append(tw); y_test.append(cls)\n",
        "\n",
        "X_train = np.vstack(X_train_list)\n",
        "y_train = np.array(y_train_list)\n",
        "X_test = np.array(X_test) if len(X_test)>0 else np.zeros((0,win_samples))\n",
        "y_test = np.array(y_test) if len(y_test)>0 else np.array([])\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "# --- encode + normalize (fit on train only) ---\n",
        "le = LabelEncoder(); le.fit(y_train); y_train_enc = le.transform(y_train)\n",
        "y_test_enc = le.transform(y_test) if len(y_test)>0 else np.array([], dtype=int)\n",
        "class_names = le.classes_\n",
        "print(\"Class order:\", class_names)\n",
        "\n",
        "X_train = X_train[..., np.newaxis]; X_test = X_test[..., np.newaxis] if X_test.shape[0]>0 else np.zeros((0,win_samples,1))\n",
        "mean = X_train.mean(); std = X_train.std()\n",
        "X_train = (X_train - mean) / (std + 1e-8)\n",
        "if X_test.shape[0]>0: X_test = (X_test - mean) / (std + 1e-8)\n",
        "\n",
        "np.save(\"mean.npy\", mean); np.save(\"std.npy\", std); np.save(\"classes.npy\", class_names)\n",
        "\n",
        "# --- model ---\n",
        "input_shape = X_train.shape[1:]\n",
        "def build_model(inp, ncls):\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=inp),\n",
        "        layers.Conv1D(32,5,activation='relu',padding='same'),\n",
        "        layers.BatchNormalization(), layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64,5,activation='relu',padding='same'),\n",
        "        layers.BatchNormalization(), layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(128,3,activation='relu',padding='same'),\n",
        "        layers.BatchNormalization(), layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(64,activation='relu'), layers.Dropout(0.3),\n",
        "        layers.Dense(ncls, activation='softmax')\n",
        "    ])\n",
        "    return m\n",
        "\n",
        "model = build_model(input_shape, len(class_names))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# --- train (quick) ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_enc, test_size=0.15, stratify=y_train_enc, random_state=random_seed)\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
        "model.fit(X_tr, y_tr, validation_data=(X_val,y_val), epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
        "\n",
        "# --- evaluate on held-out if exists ---\n",
        "if X_test.shape[0] > 0:\n",
        "    preds = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "    # Use labels = full set of class indices so report always matches class_names\n",
        "    all_label_indices = list(range(len(class_names)))\n",
        "\n",
        "    print(\"\\nClassification report (held-out):\")\n",
        "    print(classification_report(y_test_enc, preds,\n",
        "                                labels=all_label_indices,\n",
        "                                target_names=class_names,\n",
        "                                zero_division=0))\n",
        "\n",
        "    print(\"Confusion matrix (held-out):\")\n",
        "    cm = confusion_matrix(y_test_enc, preds, labels=all_label_indices)\n",
        "    print(cm)\n",
        "\n",
        "    # Show per-class support (how many held-out samples per class)\n",
        "    support = cm.sum(axis=1)\n",
        "    for i, cname in enumerate(class_names):\n",
        "        print(f\"Class {cname}: support (held-out) = {support[i]}\")\n",
        "else:\n",
        "    print(\"\\n[WARN] No held-out real windows to evaluate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "40fT-lmlQs1x",
        "outputId": "8ed5af35-0dc3-4a05-f4ac-b8b79a0dbc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window samples: 50  step: 25\n",
            "session_rest.csv rows:101 windows:3 class:rest\n",
            "session_walk.csv rows:52 windows:1 class:walk\n",
            "[INFO] only 1 window for class 'walk' — used as base, no held-out reserved.\n",
            "session_lift.csv rows:50 windows:1 class:lift\n",
            "[INFO] only 1 window for class 'lift' — used as base, no held-out reserved.\n",
            "\n",
            "Bases per class:\n",
            "  rest 2\n",
            "  walk 1\n",
            "  lift 1\n",
            "\n",
            "Held-out per class:\n",
            "  rest 1\n",
            "\n",
            "[WARN] Classes with NO held-out windows (cannot evaluate on unseen real windows): ['walk', 'lift']\n",
            "[AUG] rest: bases=2 -> target 100\n",
            "[AUG] walk: bases=1 -> target 100\n",
            "[AUG] lift: bases=1 -> target 100\n",
            "Train shape: (300, 50) Test shape: (1, 50)\n",
            "Class order: ['lift' 'rest' 'walk']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m10,304\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,547\u001b[0m (174.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,547</span> (174.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,099\u001b[0m (172.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,099</span> (172.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 - 4s - 1s/step - accuracy: 0.8863 - loss: 0.3569 - val_accuracy: 0.6667 - val_loss: 1.0231\n",
            "Epoch 2/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.0286 - val_accuracy: 0.6667 - val_loss: 0.9628\n",
            "Epoch 3/20\n",
            "4/4 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.9333 - val_loss: 0.9065\n",
            "Epoch 4/20\n",
            "4/4 - 0s - 43ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.8515\n",
            "Epoch 5/20\n",
            "4/4 - 0s - 43ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.8018\n",
            "Epoch 6/20\n",
            "4/4 - 0s - 37ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.7596\n",
            "Epoch 7/20\n",
            "4/4 - 0s - 41ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.7245\n",
            "Epoch 8/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.6955\n",
            "Epoch 9/20\n",
            "4/4 - 0s - 40ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 0.6714\n",
            "Epoch 10/20\n",
            "4/4 - 0s - 42ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 0.6504\n",
            "Epoch 11/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.8889 - val_loss: 0.6329\n",
            "Epoch 12/20\n",
            "4/4 - 0s - 75ms/step - accuracy: 1.0000 - loss: 8.3212e-04 - val_accuracy: 0.8444 - val_loss: 0.6170\n",
            "Epoch 13/20\n",
            "4/4 - 0s - 39ms/step - accuracy: 1.0000 - loss: 6.5103e-04 - val_accuracy: 0.8444 - val_loss: 0.6031\n",
            "Epoch 14/20\n",
            "4/4 - 0s - 39ms/step - accuracy: 1.0000 - loss: 9.2763e-04 - val_accuracy: 0.8222 - val_loss: 0.5909\n",
            "Epoch 15/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 7.5847e-04 - val_accuracy: 0.8000 - val_loss: 0.5803\n",
            "Epoch 16/20\n",
            "4/4 - 0s - 43ms/step - accuracy: 1.0000 - loss: 6.7481e-04 - val_accuracy: 0.7778 - val_loss: 0.5709\n",
            "Epoch 17/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 4.5069e-04 - val_accuracy: 0.7333 - val_loss: 0.5625\n",
            "Epoch 18/20\n",
            "4/4 - 0s - 37ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7333 - val_loss: 0.5548\n",
            "Epoch 19/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 5.7053e-04 - val_accuracy: 0.7333 - val_loss: 0.5475\n",
            "Epoch 20/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 3.6034e-04 - val_accuracy: 0.7333 - val_loss: 0.5406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b7b1c4eede0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
            "\n",
            "Classification report (held-out):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        lift       0.00      0.00      0.00         0\n",
            "        rest       1.00      1.00      1.00         1\n",
            "        walk       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       0.33      0.33      0.33         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "Confusion matrix (held-out):\n",
            "[[0 0 0]\n",
            " [0 1 0]\n",
            " [0 0 0]]\n",
            "Class lift: support (held-out) = 0\n",
            "Class rest: support (held-out) = 1\n",
            "Class walk: support (held-out) = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# emg_full_pipeline_safe.py\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# -----------------------\n",
        "# USER PARAMETERS (edit if needed)\n",
        "# -----------------------\n",
        "csv_paths = [\"data/session_rest.csv\", \"data/session_walk.csv\", \"data/session_lift.csv\"]\n",
        "emg_col = \"emg\"\n",
        "label_col = \"label\"      # if missing in CSV, label inferred from filename\n",
        "sampling_rate = 1000     # Hz\n",
        "window_ms = 50           # window length in ms\n",
        "step_ms = 25             # step in ms\n",
        "target_windows_per_class = 100   # augment to this many windows per class (reduce to 50 for speed)\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "random_seed = 42\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "def infer_label_from_filename(path):\n",
        "    fname = os.path.basename(path).lower()\n",
        "    if \"rest\" in fname: return \"rest\"\n",
        "    if \"walk\" in fname: return \"walk\"\n",
        "    if \"lift\" in fname or \"flex\" in fname: return \"lift\"\n",
        "    return os.path.splitext(fname)[0]\n",
        "\n",
        "def load_single_file(path):\n",
        "    df = pd.read_csv(path)\n",
        "    if label_col not in df.columns or df[label_col].isna().all():\n",
        "        df[label_col] = infer_label_from_filename(path)\n",
        "    return df\n",
        "\n",
        "def build_windows_from_array(arr_emg, win_samples, step_samples):\n",
        "    outs = []\n",
        "    n = len(arr_emg)\n",
        "    for start in range(0, n - win_samples + 1, step_samples):\n",
        "        outs.append(arr_emg[start:start+win_samples].astype(np.float32))\n",
        "    return np.array(outs)\n",
        "\n",
        "def augment_window(window):\n",
        "    w = window.copy()\n",
        "    # small random shift (jitter)\n",
        "    shift = np.random.randint(-4, 5)\n",
        "    if shift > 0:\n",
        "        w = np.concatenate([w[shift:], np.full(shift, w[-1], dtype=w.dtype)])\n",
        "    elif shift < 0:\n",
        "        s = -shift\n",
        "        w = np.concatenate([np.full(s, w[0], dtype=w.dtype), w[:-s]])\n",
        "    # amplitude scale\n",
        "    w = w * np.random.uniform(0.95, 1.05)\n",
        "    # additive noise proportional to signal range\n",
        "    rng = np.max(w) - np.min(w)\n",
        "    w = w + np.random.normal(0.0, 0.02 * (rng + 1e-8), size=w.shape)\n",
        "    return w\n",
        "\n",
        "def create_augmented_windows(bases, target):\n",
        "    \"\"\"Given bases (n_base, win_samples), produce 'target' windows by augmentation.\"\"\"\n",
        "    if len(bases) == 0:\n",
        "        return np.zeros((0,0), dtype=np.float32)\n",
        "    out = [b for b in bases]\n",
        "    while len(out) < target:\n",
        "        b = bases[np.random.randint(0, len(bases))]\n",
        "        out.append(augment_window(b))\n",
        "    return np.array(out, dtype=np.float32)\n",
        "\n",
        "def safe_label_encode_fit(y_train):\n",
        "    \"\"\"Fit LabelEncoder on y_train and return mapping dict and encoder.\"\"\"\n",
        "    le = LabelEncoder()\n",
        "    le.fit(y_train)\n",
        "    classes = list(le.classes_)\n",
        "    label_to_idx = {c: int(i) for i,c in enumerate(classes)}\n",
        "    return le, classes, label_to_idx\n",
        "\n",
        "def safe_label_transform(labels, label_to_idx):\n",
        "    \"\"\"Transform labels into indices using label_to_idx; unseen labels -> dropped (with warning).\"\"\"\n",
        "    out_idx = []\n",
        "    keep_mask = []\n",
        "    unseen = set()\n",
        "    for v in labels:\n",
        "        if v in label_to_idx:\n",
        "            out_idx.append(label_to_idx[v])\n",
        "            keep_mask.append(True)\n",
        "        else:\n",
        "            unseen.add(v)\n",
        "            keep_mask.append(False)\n",
        "    if unseen:\n",
        "        print(\"[WARN] Dropping samples with unseen labels:\", unseen)\n",
        "    return np.array(out_idx, dtype=int), np.array(keep_mask, dtype=bool)\n",
        "\n",
        "# -----------------------\n",
        "# Windowing & holdout reservation (flexible)\n",
        "# - reserve last window as held-out only if file has >= 2 windows\n",
        "# - if file has exactly 1 window, use it as augmentation base (no held-out)\n",
        "# -----------------------\n",
        "win_samples = int(sampling_rate * window_ms / 1000)\n",
        "step_samples = int(sampling_rate * step_ms / 1000)\n",
        "if win_samples <= 0:\n",
        "    raise ValueError(\"Invalid window parameters. win_samples <= 0\")\n",
        "\n",
        "print(\"Window samples:\", win_samples, \" step:\", step_samples)\n",
        "\n",
        "per_class_bases = {}   # class -> list of windows (bases for augmentation)\n",
        "per_class_heldout = {} # class -> list of windows reserved for test (no augmentation)\n",
        "\n",
        "for p in csv_paths:\n",
        "    if not os.path.exists(p):\n",
        "        print(f\"[WARN] File not found: {p} (skipping)\")\n",
        "        continue\n",
        "    df = load_single_file(p)\n",
        "    cls = str(df[label_col].iloc[0])\n",
        "    arr = df[emg_col].values\n",
        "    wins = build_windows_from_array(arr, win_samples, step_samples)\n",
        "    print(f\"{os.path.basename(p)} rows:{len(arr)} windows:{len(wins)} class:{cls}\")\n",
        "    if len(wins) == 0:\n",
        "        continue\n",
        "    if len(wins) >= 2:\n",
        "        # reserve last window as held-out test, use the rest as augmentation bases\n",
        "        per_class_heldout.setdefault(cls, []).append(wins[-1])\n",
        "        per_class_bases.setdefault(cls, []).extend(list(wins[:-1]))\n",
        "    else:\n",
        "        # exactly 1 window: use as base only (no held-out)\n",
        "        per_class_bases.setdefault(cls, []).extend(list(wins))\n",
        "        print(f\"[INFO] only 1 window for class '{cls}' — used as base, no held-out reserved.\")\n",
        "\n",
        "print(\"\\nBases per class:\")\n",
        "for c, arr in per_class_bases.items():\n",
        "    print(\" \", c, len(arr))\n",
        "print(\"\\nHeld-out per class:\")\n",
        "for c, arr in per_class_heldout.items():\n",
        "    print(\" \", c, len(arr))\n",
        "\n",
        "classes_without_heldout = [c for c in per_class_bases.keys() if c not in per_class_heldout]\n",
        "if classes_without_heldout:\n",
        "    print(\"\\n[WARN] Classes without held-out windows (cannot evaluate unseen real windows for these):\", classes_without_heldout)\n",
        "\n",
        "# -----------------------\n",
        "# Create augmented training data (balanced per-class)\n",
        "# -----------------------\n",
        "X_train_parts = []\n",
        "y_train_parts = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for cls, bases in per_class_bases.items():\n",
        "    bases_np = np.array(bases) if len(bases) > 0 else np.zeros((0, win_samples))\n",
        "    print(f\"[AUG] class {cls}: bases={len(bases_np)} -> augment to {target_windows_per_class}\")\n",
        "    augmented = create_augmented_windows(bases_np, target_windows_per_class)\n",
        "    if augmented.shape[0] > 0:\n",
        "        X_train_parts.append(augmented)\n",
        "        y_train_parts.extend([cls] * len(augmented))\n",
        "    # attach any reserved held-out windows for that class (may be empty)\n",
        "    for tw in per_class_heldout.get(cls, []):\n",
        "        X_test.append(tw)\n",
        "        y_test.append(cls)\n",
        "\n",
        "if len(X_train_parts) == 0:\n",
        "    raise RuntimeError(\"No training data (no bases produced). Collect more recordings or adjust window params.\")\n",
        "\n",
        "X_train = np.vstack(X_train_parts)\n",
        "y_train = np.array(y_train_parts)\n",
        "X_test = np.array(X_test) if len(X_test) > 0 else np.zeros((0, win_samples))\n",
        "y_test = np.array(y_test) if len(y_test) > 0 else np.array([])\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "print(\"Train class counts:\", {c: sum(y_train == c) for c in np.unique(y_train)} )\n",
        "print(\"Test class counts:\", {c: sum(y_test == c) for c in np.unique(y_test)} )\n",
        "\n",
        "# -----------------------\n",
        "# Safe label encoding (fit on train only)\n",
        "# -----------------------\n",
        "le, class_names, label_to_idx = safe_label_encode_fit(y_train)\n",
        "print(\"Class order (training):\", class_names)\n",
        "\n",
        "# transform training labels\n",
        "y_train_enc = np.array([label_to_idx[v] for v in y_train], dtype=int)\n",
        "\n",
        "# transform test labels safely (drop unseen)\n",
        "if X_test.shape[0] > 0:\n",
        "    y_test_enc, keep_mask = safe_label_transform(y_test, label_to_idx)\n",
        "    if not keep_mask.all():\n",
        "        X_test = X_test[keep_mask]\n",
        "        y_test = y_test[keep_mask]\n",
        "else:\n",
        "    y_test_enc = np.array([], dtype=int)\n",
        "\n",
        "# -----------------------\n",
        "# Preprocessing: reshape and normalize (fit mean/std on train only)\n",
        "# -----------------------\n",
        "X_train = X_train[..., np.newaxis]  # (N, win_samples, 1)\n",
        "X_test = X_test[..., np.newaxis] if X_test.shape[0] > 0 else np.zeros((0, win_samples, 1))\n",
        "\n",
        "mean = float(X_train.mean())\n",
        "std  = float(X_train.std())\n",
        "print(\"Train mean/std:\", mean, std)\n",
        "\n",
        "# Normalize\n",
        "X_train = (X_train - mean) / (std + 1e-8)\n",
        "if X_test.shape[0] > 0:\n",
        "    X_test = (X_test - mean) / (std + 1e-8)\n",
        "\n",
        "# Save preprocessing artifacts\n",
        "np.save(\"mean_inferred.npy\", mean)\n",
        "np.save(\"std_inferred.npy\", std)\n",
        "np.save(\"classes_inferred.npy\", np.array(class_names, dtype=object))\n",
        "print(\"Saved mean_inferred.npy, std_inferred.npy, classes_inferred.npy\")\n",
        "\n",
        "# -----------------------\n",
        "# Build model (compact 1D-CNN)\n",
        "# -----------------------\n",
        "input_shape = X_train.shape[1:]\n",
        "def build_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv1D(32, 5, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64, 5, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(input_shape, len(class_names))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# -----------------------\n",
        "# Train (with a small val split)\n",
        "# -----------------------\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_enc, test_size=0.15, random_state=random_seed, stratify=y_train_enc)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "]\n",
        "history = model.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate on held-out real windows (if any)\n",
        "# -----------------------\n",
        "if X_test.shape[0] > 0:\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_enc, verbose=0)\n",
        "    print(f\"\\nKeras held-out evaluation -> loss: {test_loss:.4f} acc: {test_acc:.4f}\")\n",
        "    preds = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "    # safe full-class report: supply labels=list(range(n_classes))\n",
        "    labels_full = list(range(len(class_names)))\n",
        "    print(\"\\nClassification report (held-out):\")\n",
        "    print(classification_report(y_test_enc, preds, labels=labels_full, target_names=class_names, zero_division=0))\n",
        "    print(\"Confusion matrix (held-out):\")\n",
        "    print(confusion_matrix(y_test_enc, preds, labels=labels_full))\n",
        "\n",
        "    # per-class support in held-out\n",
        "    cm = confusion_matrix(y_test_enc, preds, labels=labels_full)\n",
        "    for i, cname in enumerate(class_names):\n",
        "        print(f\"Class {cname}: held-out support = {cm.sum(axis=1)[i]}\")\n",
        "else:\n",
        "    print(\"\\n[WARN] No held-out real windows were available to evaluate. Collect additional recordings to measure real generalization.\")\n",
        "\n",
        "# -----------------------\n",
        "# Save Keras model and convert to TFLite\n",
        "# -----------------------\n",
        "model.save(\"final_emg_model_safe.h5\")\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(\"emg_model_safe.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "print(\"Saved final_emg_model_safe.h5 and emg_model_safe.tflite\")\n",
        "\n",
        "# -----------------------\n",
        "# Optional: Quick TFLite parity check on held-out (if present)\n",
        "# -----------------------\n",
        "if X_test.shape[0] > 0:\n",
        "    interpreter = tf.lite.Interpreter(model_path=\"emg_model_safe.tflite\")\n",
        "    interpreter.allocate_tensors()\n",
        "    in_d = interpreter.get_input_details()[0]\n",
        "    out_d = interpreter.get_output_details()[0]\n",
        "    preds_tflite = []\n",
        "    for w in X_test:\n",
        "        x = w.astype(np.float32)\n",
        "        # if quantized input expected, scaling would be needed; here model is float\n",
        "        interpreter.set_tensor(in_d['index'], x.reshape(1, x.shape[0], 1).astype(np.float32))\n",
        "        interpreter.invoke()\n",
        "        out = interpreter.get_tensor(out_d['index'])[0]\n",
        "        preds_tflite.append(np.argmax(out))\n",
        "    preds_tflite = np.array(preds_tflite)\n",
        "    print(\"\\nTFLite held-out accuracy:\", accuracy_score(y_test_enc, preds_tflite))\n",
        "    print(\"TFLite confusion:\\n\", confusion_matrix(y_test_enc, preds_tflite, labels=labels_full))\n",
        "\n",
        "print(\"\\nFinished pipeline.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uma0FblBRWTG",
        "outputId": "9380e98b-91e5-4957-eb85-6cd11087ff83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window samples: 50  step: 25\n",
            "session_rest.csv rows:101 windows:3 class:rest\n",
            "session_walk.csv rows:52 windows:1 class:walk\n",
            "[INFO] only 1 window for class 'walk' — used as base, no held-out reserved.\n",
            "session_lift.csv rows:50 windows:1 class:lift\n",
            "[INFO] only 1 window for class 'lift' — used as base, no held-out reserved.\n",
            "\n",
            "Bases per class:\n",
            "  rest 2\n",
            "  walk 1\n",
            "  lift 1\n",
            "\n",
            "Held-out per class:\n",
            "  rest 1\n",
            "\n",
            "[WARN] Classes without held-out windows (cannot evaluate unseen real windows for these): ['walk', 'lift']\n",
            "[AUG] class rest: bases=2 -> augment to 100\n",
            "[AUG] class walk: bases=1 -> augment to 100\n",
            "[AUG] class lift: bases=1 -> augment to 100\n",
            "Train shape: (300, 50) Test shape: (1, 50)\n",
            "Train class counts: {np.str_('lift'): np.int64(100), np.str_('rest'): np.int64(100), np.str_('walk'): np.int64(100)}\n",
            "Test class counts: {np.str_('rest'): np.int64(1)}\n",
            "Class order (training): [np.str_('lift'), np.str_('rest'), np.str_('walk')]\n",
            "Train mean/std: 774.6346435546875 230.83572387695312\n",
            "Saved mean_inferred.npy, std_inferred.npy, classes_inferred.npy\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m10,304\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,547\u001b[0m (174.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,547</span> (174.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,099\u001b[0m (172.26 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,099</span> (172.26 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 - 3s - 857ms/step - accuracy: 0.8941 - loss: 0.3525 - val_accuracy: 0.6667 - val_loss: 1.0173\n",
            "Epoch 2/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.0299 - val_accuracy: 0.6667 - val_loss: 0.9505\n",
            "Epoch 3/20\n",
            "4/4 - 0s - 60ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9333 - val_loss: 0.8900\n",
            "Epoch 4/20\n",
            "4/4 - 0s - 61ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.8329\n",
            "Epoch 5/20\n",
            "4/4 - 0s - 77ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.7819\n",
            "Epoch 6/20\n",
            "4/4 - 0s - 57ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.7387\n",
            "Epoch 7/20\n",
            "4/4 - 0s - 77ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 0.7030\n",
            "Epoch 8/20\n",
            "4/4 - 0s - 60ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.6743\n",
            "Epoch 9/20\n",
            "4/4 - 0s - 64ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.6513\n",
            "Epoch 10/20\n",
            "4/4 - 0s - 74ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9778 - val_loss: 0.6321\n",
            "Epoch 11/20\n",
            "4/4 - 0s - 57ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9556 - val_loss: 0.6161\n",
            "Epoch 12/20\n",
            "4/4 - 0s - 39ms/step - accuracy: 1.0000 - loss: 8.8624e-04 - val_accuracy: 0.8667 - val_loss: 0.6026\n",
            "Epoch 13/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 9.4837e-04 - val_accuracy: 0.8444 - val_loss: 0.5907\n",
            "Epoch 14/20\n",
            "4/4 - 0s - 39ms/step - accuracy: 1.0000 - loss: 9.1217e-04 - val_accuracy: 0.7556 - val_loss: 0.5807\n",
            "Epoch 15/20\n",
            "4/4 - 0s - 42ms/step - accuracy: 1.0000 - loss: 8.4968e-04 - val_accuracy: 0.7556 - val_loss: 0.5722\n",
            "Epoch 16/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 5.7883e-04 - val_accuracy: 0.7333 - val_loss: 0.5648\n",
            "Epoch 17/20\n",
            "4/4 - 0s - 40ms/step - accuracy: 1.0000 - loss: 4.8227e-04 - val_accuracy: 0.7333 - val_loss: 0.5581\n",
            "Epoch 18/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.7111 - val_loss: 0.5519\n",
            "Epoch 19/20\n",
            "4/4 - 0s - 39ms/step - accuracy: 1.0000 - loss: 6.2404e-04 - val_accuracy: 0.7111 - val_loss: 0.5462\n",
            "Epoch 20/20\n",
            "4/4 - 0s - 38ms/step - accuracy: 1.0000 - loss: 4.0753e-04 - val_accuracy: 0.6667 - val_loss: 0.5408\n",
            "\n",
            "Keras held-out evaluation -> loss: 0.3464 acc: 1.0000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification report (held-out):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        lift       0.00      0.00      0.00         0\n",
            "        rest       1.00      1.00      1.00         1\n",
            "        walk       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       0.33      0.33      0.33         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "Confusion matrix (held-out):\n",
            "[[0 0 0]\n",
            " [0 1 0]\n",
            " [0 0 0]]\n",
            "Class lift: held-out support = 0\n",
            "Class rest: held-out support = 1\n",
            "Class walk: held-out support = 0\n",
            "Saved artifact at '/tmp/tmprtuwt639'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 50, 1), dtype=tf.float32, name='keras_tensor_52')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135768687631184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687629648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687633488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687637904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687632720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687632336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687632528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687639440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687631760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768684091472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687638672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687631376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768684088400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768684091088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687635600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687635984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687633680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768687635792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768628561104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768628561680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768628563792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135768628563216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Saved final_emg_model_safe.h5 and emg_model_safe.tflite\n",
            "\n",
            "TFLite held-out accuracy: 1.0\n",
            "TFLite confusion:\n",
            " [[0 0 0]\n",
            " [0 1 0]\n",
            " [0 0 0]]\n",
            "\n",
            "Finished pipeline.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lofo_quick.py  -- quick LOFO evaluation (fast)\n",
        "import os, random, numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# --- USER (edit if you'd like) ---\n",
        "csv_files = [\"data/session_rest.csv\", \"data/session_walk.csv\", \"data/session_lift.csv\"]\n",
        "emg_col = \"emg\"\n",
        "sampling_rate = 1000\n",
        "window_ms = 50\n",
        "step_ms = 25\n",
        "target_windows_per_class = 50   # smaller -> faster\n",
        "batch_size = 64\n",
        "epochs = 10                     # fewer epochs -> faster\n",
        "random_seed = 42\n",
        "\n",
        "np.random.seed(random_seed); random.seed(random_seed); tf.random.set_seed(random_seed)\n",
        "\n",
        "# --- helpers ---\n",
        "def infer_label_from_filename(path):\n",
        "    fn = os.path.basename(path).lower()\n",
        "    if \"rest\" in fn: return \"rest\"\n",
        "    if \"walk\" in fn: return \"walk\"\n",
        "    if \"lift\" in fn or \"flex\" in fn: return \"lift\"\n",
        "    return os.path.splitext(fn)[0]\n",
        "\n",
        "def load_windows(path, win_samples, step_samples):\n",
        "    df = pd.read_csv(path)\n",
        "    if 'label' not in df.columns or df['label'].isna().all():\n",
        "        df['label'] = infer_label_from_filename(path)\n",
        "    arr = df[emg_col].values\n",
        "    outs=[]\n",
        "    for s in range(0, len(arr)-win_samples+1, step_samples):\n",
        "        outs.append(arr[s:s+win_samples].astype(np.float32))\n",
        "    return np.array(outs), df['label'].iloc[0]\n",
        "\n",
        "def augment_window(w):\n",
        "    w = w.copy()\n",
        "    shift = np.random.randint(-4,5)\n",
        "    if shift>0: w = np.concatenate([w[shift:], np.full(shift, w[-1], dtype=w.dtype)])\n",
        "    elif shift<0:\n",
        "        s=-shift; w = np.concatenate([np.full(s, w[0], dtype=w.dtype), w[:-s]])\n",
        "    w = w * np.random.uniform(0.95, 1.05)\n",
        "    rng = np.max(w)-np.min(w)\n",
        "    w = w + np.random.normal(0, 0.02*(rng+1e-8), size=w.shape)\n",
        "    return w\n",
        "\n",
        "def create_augmented(bases, target):\n",
        "    if len(bases)==0: return np.zeros((0,0), dtype=np.float32)\n",
        "    out = [b for b in bases]\n",
        "    while len(out) < target:\n",
        "        b = bases[np.random.randint(0, len(bases))]\n",
        "        out.append(augment_window(b))\n",
        "    return np.array(out, dtype=np.float32)\n",
        "\n",
        "# --- window params ---\n",
        "win_samples = int(sampling_rate * window_ms / 1000)\n",
        "step_samples = int(sampling_rate * step_ms / 1000)\n",
        "if win_samples <= 0: raise SystemExit(\"check window params\")\n",
        "\n",
        "fold_results = []\n",
        "for i, test_file in enumerate(csv_files):\n",
        "    print(f\"\\n=== FOLD {i+1}/{len(csv_files)}: test = {os.path.basename(test_file)} ===\")\n",
        "    train_bases = {}\n",
        "    test_windows = None\n",
        "    test_label = None\n",
        "\n",
        "    for f in csv_files:\n",
        "        wins, label = load_windows(f, win_samples, step_samples)\n",
        "        if f == test_file:\n",
        "            test_windows = wins\n",
        "            test_label = label\n",
        "            print(\" Test windows:\", len(test_windows), \" label:\", test_label)\n",
        "        else:\n",
        "            if len(wins) > 0:\n",
        "                train_bases.setdefault(label, []).extend(list(wins))\n",
        "                print(\" Train base added:\", os.path.basename(f), \"wins:\", len(wins), \"label:\", label)\n",
        "\n",
        "    # skip if missing train data\n",
        "    if len(train_bases) == 0:\n",
        "        print(\"[SKIP] No training bases for this fold\"); continue\n",
        "    if test_windows is None or len(test_windows)==0:\n",
        "        print(\"[SKIP] No test windows in test file\"); continue\n",
        "\n",
        "    # build augmented train set\n",
        "    X_train_parts = []; y_train_parts = []\n",
        "    for cls, bases in train_bases.items():\n",
        "        bases_np = np.array(bases)\n",
        "        aug = create_augmented(bases_np, target_windows_per_class)\n",
        "        if aug.shape[0] > 0:\n",
        "            X_train_parts.append(aug)\n",
        "            y_train_parts.extend([cls]*len(aug))\n",
        "    if len(X_train_parts) == 0:\n",
        "        print(\"[SKIP] No augmented train data\"); continue\n",
        "\n",
        "    X_train = np.vstack(X_train_parts); y_train = np.array(y_train_parts)\n",
        "    X_test = test_windows; y_test = np.array([test_label]*len(X_test))\n",
        "\n",
        "    # encode\n",
        "    le = LabelEncoder(); le.fit(y_train)\n",
        "    y_train_enc = le.transform(y_train)\n",
        "    # if test contains unseen class, skip fold (shouldn't happen with LOFO)\n",
        "    try:\n",
        "        y_test_enc = le.transform(y_test)\n",
        "    except ValueError:\n",
        "        print(\"[SKIP] Test contains unseen labels compared to train\"); continue\n",
        "\n",
        "    # reshape + normalize (fit on train only)\n",
        "    X_train = X_train[..., np.newaxis]; X_test = X_test[..., np.newaxis]\n",
        "    mean = X_train.mean(); std = X_train.std()\n",
        "    X_train = (X_train - mean) / (std + 1e-8)\n",
        "    X_test  = (X_test - mean) / (std + 1e-8)\n",
        "\n",
        "    # model (smaller for speed)\n",
        "    def build_model(inp, ncls):\n",
        "        m = models.Sequential([\n",
        "            layers.Input(shape=inp),\n",
        "            layers.Conv1D(32,5,activation='relu',padding='same'),\n",
        "            layers.BatchNormalization(), layers.MaxPooling1D(2),\n",
        "            layers.Conv1D(64,5,activation='relu',padding='same'),\n",
        "            layers.BatchNormalization(), layers.GlobalAveragePooling1D(),\n",
        "            layers.Dense(64,activation='relu'), layers.Dropout(0.3),\n",
        "            layers.Dense(ncls, activation='softmax')\n",
        "        ])\n",
        "        return m\n",
        "\n",
        "    model = build_model(X_train.shape[1:], len(le.classes_))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train_enc, test_size=0.15, stratify=y_train_enc, random_state=random_seed)\n",
        "    model.fit(X_tr, y_tr, validation_data=(X_val,y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    preds = np.argmax(model.predict(X_test), axis=1)\n",
        "    acc = accuracy_score(y_test_enc, preds)\n",
        "    print(f\" Fold accuracy: {acc:.4f}  (n_test={len(X_test)})\")\n",
        "    print(classification_report(y_test_enc, preds, zero_division=0, target_names=le.classes_))\n",
        "    print(\" Confusion:\\n\", confusion_matrix(y_test_enc, preds))\n",
        "    fold_results.append(acc)\n",
        "\n",
        "# summary\n",
        "if fold_results:\n",
        "    print(\"\\nLOFO mean accuracy:\", np.mean(fold_results), \"per-fold:\", fold_results)\n",
        "else:\n",
        "    print(\"No folds completed successfully. Collect more windows per file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slYhyY99VXmp",
        "outputId": "dc0d44f2-0b01-4f52-cdc3-ac3b76f13f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FOLD 1/3: test = session_rest.csv ===\n",
            " Test windows: 3  label: rest\n",
            " Train base added: session_walk.csv wins: 1 label: walk\n",
            " Train base added: session_lift.csv wins: 1 label: lift\n",
            "[SKIP] Test contains unseen labels compared to train\n",
            "\n",
            "=== FOLD 2/3: test = session_walk.csv ===\n",
            " Train base added: session_rest.csv wins: 3 label: rest\n",
            " Test windows: 1  label: walk\n",
            " Train base added: session_lift.csv wins: 1 label: lift\n",
            "[SKIP] Test contains unseen labels compared to train\n",
            "\n",
            "=== FOLD 3/3: test = session_lift.csv ===\n",
            " Train base added: session_rest.csv wins: 3 label: rest\n",
            " Train base added: session_walk.csv wins: 1 label: walk\n",
            " Test windows: 1  label: lift\n",
            "[SKIP] Test contains unseen labels compared to train\n",
            "No folds completed successfully. Collect more windows per file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quick_stratified_eval.py  -- optimistic but immediate evaluation (not file-held-out)\n",
        "import os, random, numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# --- USER ---\n",
        "csv_paths = [\"data/session_rest.csv\", \"data/session_walk.csv\", \"data/session_lift.csv\"]\n",
        "emg_col = \"emg\"\n",
        "sampling_rate = 1000\n",
        "window_ms = 50\n",
        "step_ms = 25\n",
        "target_windows_per_class = 100   # lower to 50 to speed up\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "random_seed = 42\n",
        "\n",
        "np.random.seed(random_seed); random.seed(random_seed); tf.random.set_seed(random_seed)\n",
        "\n",
        "def infer_label_from_filename(path):\n",
        "    fn = os.path.basename(path).lower()\n",
        "    if \"rest\" in fn: return \"rest\"\n",
        "    if \"walk\" in fn: return \"walk\"\n",
        "    if \"lift\" in fn or \"flex\" in fn: return \"lift\"\n",
        "    return os.path.splitext(fn)[0]\n",
        "\n",
        "def build_windows(path, win_samples, step_samples):\n",
        "    df = pd.read_csv(path)\n",
        "    label = df['label'].iloc[0] if 'label' in df.columns and not df['label'].isna().all() else infer_label_from_filename(path)\n",
        "    arr = df[emg_col].values\n",
        "    outs = []\n",
        "    for s in range(0, len(arr)-win_samples+1, step_samples):\n",
        "        outs.append(arr[s:s+win_samples].astype(np.float32))\n",
        "    return np.array(outs), label\n",
        "\n",
        "def augment_window(w):\n",
        "    w = w.copy()\n",
        "    shift = np.random.randint(-4,5)\n",
        "    if shift>0: w = np.concatenate([w[shift:], np.full(shift, w[-1], dtype=w.dtype)])\n",
        "    elif shift<0:\n",
        "        s=-shift; w = np.concatenate([np.full(s, w[0], dtype=w.dtype), w[:-s]])\n",
        "    w = w * np.random.uniform(0.95, 1.05)\n",
        "    rng = np.max(w)-np.min(w)\n",
        "    w = w + np.random.normal(0, 0.02*(rng+1e-8), size=w.shape)\n",
        "    return w\n",
        "\n",
        "def create_augmented(bases, target):\n",
        "    if len(bases)==0: return np.zeros((0,0), dtype=np.float32)\n",
        "    out = [b for b in bases]\n",
        "    while len(out) < target:\n",
        "        b = bases[np.random.randint(0, len(bases))]\n",
        "        out.append(augment_window(b))\n",
        "    return np.array(out, dtype=np.float32)\n",
        "\n",
        "win_samples = int(sampling_rate * window_ms / 1000)\n",
        "step_samples = int(sampling_rate * step_ms / 1000)\n",
        "\n",
        "# collect real windows per file then aggregate by class\n",
        "per_class_bases = {}\n",
        "for p in csv_paths:\n",
        "    if not os.path.exists(p): continue\n",
        "    wins, lbl = build_windows(p, win_samples, step_samples)\n",
        "    per_class_bases.setdefault(lbl, []).extend(list(wins))\n",
        "    print(f\"{os.path.basename(p)} -> {len(wins)} windows label={lbl}\")\n",
        "\n",
        "# augment\n",
        "X_list=[]; y_list=[]\n",
        "for cls, bases in per_class_bases.items():\n",
        "    bases_np = np.array(bases)\n",
        "    if bases_np.shape[0]==0: continue\n",
        "    aug = create_augmented(bases_np, target_windows_per_class)\n",
        "    X_list.append(aug)\n",
        "    y_list.extend([cls]*len(aug))\n",
        "X = np.vstack(X_list)\n",
        "y = np.array(y_list)\n",
        "print(\"Total windows (augmented):\", X.shape, \"classes:\", np.unique(y))\n",
        "\n",
        "# encode\n",
        "le = LabelEncoder(); y_enc = le.fit_transform(y); class_names = le.classes_\n",
        "print(\"Class order:\", class_names)\n",
        "\n",
        "# reshape & normalize\n",
        "X = X[..., np.newaxis]\n",
        "mean = X.mean(); std = X.std()\n",
        "X = (X - mean) / (std + 1e-8)\n",
        "\n",
        "# stratified split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y_enc, test_size=0.25, random_state=random_seed, stratify=y_enc)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_seed, stratify=y_temp)\n",
        "\n",
        "print(\"Shapes -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
        "\n",
        "# build model\n",
        "def build_model(input_shape, ncls):\n",
        "    m = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv1D(32,5,activation='relu',padding='same'),\n",
        "        layers.BatchNormalization(), layers.MaxPooling1D(2),\n",
        "        layers.Conv1D(64,5,activation='relu',padding='same'),\n",
        "        layers.BatchNormalization(), layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(64,activation='relu'), layers.Dropout(0.3),\n",
        "        layers.Dense(ncls, activation='softmax')\n",
        "    ])\n",
        "    return m\n",
        "\n",
        "model = build_model(X_train.shape[1:], len(class_names))\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_v, y_tr, y_v = train_test_split(X_train, y_train, test_size=0.15, random_state=random_seed, stratify=y_train)\n",
        "model.fit(X_tr, y_tr, validation_data=(X_v,y_v), epochs=epochs, batch_size=batch_size, verbose=2)\n",
        "\n",
        "# evaluate\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(\"Overall accuracy (optimistic):\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))\n",
        "print(\"Confusion:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeKp98Z1W8kG",
        "outputId": "79e3ab0e-6a43-48d3-9918-0cb71db3669a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "session_rest.csv -> 3 windows label=rest\n",
            "session_walk.csv -> 1 windows label=walk\n",
            "session_lift.csv -> 1 windows label=lift\n",
            "Total windows (augmented): (300, 50) classes: ['lift' 'rest' 'walk']\n",
            "Class order: ['lift' 'rest' 'walk']\n",
            "Shapes -> train: (225, 50, 1) val: (37, 50, 1) test: (38, 50, 1)\n",
            "Epoch 1/20\n",
            "3/3 - 3s - 871ms/step - accuracy: 0.4398 - loss: 1.0856 - val_accuracy: 0.6471 - val_loss: 1.0505\n",
            "Epoch 2/20\n",
            "3/3 - 0s - 39ms/step - accuracy: 0.9948 - loss: 0.3421 - val_accuracy: 0.6471 - val_loss: 0.9990\n",
            "Epoch 3/20\n",
            "3/3 - 0s - 38ms/step - accuracy: 0.9948 - loss: 0.1734 - val_accuracy: 0.6471 - val_loss: 0.9428\n",
            "Epoch 4/20\n",
            "3/3 - 0s - 44ms/step - accuracy: 0.9948 - loss: 0.0973 - val_accuracy: 0.7647 - val_loss: 0.8887\n",
            "Epoch 5/20\n",
            "3/3 - 0s - 44ms/step - accuracy: 1.0000 - loss: 0.0585 - val_accuracy: 0.8824 - val_loss: 0.8397\n",
            "Epoch 6/20\n",
            "3/3 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.0384 - val_accuracy: 0.9412 - val_loss: 0.7941\n",
            "Epoch 7/20\n",
            "3/3 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.0252 - val_accuracy: 0.9706 - val_loss: 0.7531\n",
            "Epoch 8/20\n",
            "3/3 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.0187 - val_accuracy: 1.0000 - val_loss: 0.7163\n",
            "Epoch 9/20\n",
            "3/3 - 0s - 40ms/step - accuracy: 1.0000 - loss: 0.0220 - val_accuracy: 1.0000 - val_loss: 0.6832\n",
            "Epoch 10/20\n",
            "3/3 - 0s - 40ms/step - accuracy: 1.0000 - loss: 0.0139 - val_accuracy: 1.0000 - val_loss: 0.6530\n",
            "Epoch 11/20\n",
            "3/3 - 0s - 40ms/step - accuracy: 1.0000 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 0.6260\n",
            "Epoch 12/20\n",
            "3/3 - 0s - 53ms/step - accuracy: 1.0000 - loss: 0.0121 - val_accuracy: 1.0000 - val_loss: 0.6022\n",
            "Epoch 13/20\n",
            "3/3 - 0s - 86ms/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 0.5810\n",
            "Epoch 14/20\n",
            "3/3 - 0s - 40ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.5620\n",
            "Epoch 15/20\n",
            "3/3 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.5447\n",
            "Epoch 16/20\n",
            "3/3 - 0s - 41ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 0.5285\n",
            "Epoch 17/20\n",
            "3/3 - 0s - 40ms/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.5134\n",
            "Epoch 18/20\n",
            "3/3 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.4989\n",
            "Epoch 19/20\n",
            "3/3 - 0s - 54ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.4850\n",
            "Epoch 20/20\n",
            "3/3 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.4719\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "Overall accuracy (optimistic): 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        lift       1.00      1.00      1.00        12\n",
            "        rest       1.00      1.00      1.00        13\n",
            "        walk       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        38\n",
            "   macro avg       1.00      1.00      1.00        38\n",
            "weighted avg       1.00      1.00      1.00        38\n",
            "\n",
            "Confusion:\n",
            " [[12  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0sJYhsOiXbQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}